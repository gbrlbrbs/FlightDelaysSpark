{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1eab0f1d-e52c-46a8-81f3-65f163949601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading airline-delay-and-cancellation-data-2009-2018.zip to /opt/workspace/data/airline-delay-and-cancellation-data-2009-2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.95G/1.95G [03:56<00:00, 8.84MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from get_data import get_data\n",
    "\n",
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "864741eb-638f-46d0-89c9-ebef4b69f6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, FloatType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_PATH = Path(\"/opt/workspace/data\")\n",
    "csv_files = list(DATA_PATH.glob(\"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "755819a0-ba32-44fd-9934-4ebbeaf5302d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/06 10:02:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "jar_postgres = Path.cwd() / \"postgresql-42.5.1.jar\"\n",
    "master_url = \"spark://spark-master:7077\"\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"FlightDelaysNotebook\") \\\n",
    "        .config(\"spark.jars\", str(jar_postgres)) \\\n",
    "        .master(master_url) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d77ace-3051-488c-bebf-d4c34103c5e8",
   "metadata": {},
   "source": [
    "Lendo os dados baixados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a465698-a2d5-4d99-a7a4-984c6743aabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(list(map(lambda x: str(x), csv_files)), header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c10c2702-fe9d-406a-93fb-ba52c7556dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- OP_CARRIER: string (nullable = true)\n",
      " |-- OP_CARRIER_FL_NUM: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEP_DELAY: string (nullable = true)\n",
      " |-- TAXI_OUT: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      " |-- TAXI_IN: string (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- ARR_DELAY: string (nullable = true)\n",
      " |-- CANCELLED: string (nullable = true)\n",
      " |-- CANCELLATION_CODE: string (nullable = true)\n",
      " |-- DIVERTED: string (nullable = true)\n",
      " |-- CRS_ELAPSED_TIME: string (nullable = true)\n",
      " |-- ACTUAL_ELAPSED_TIME: string (nullable = true)\n",
      " |-- AIR_TIME: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- CARRIER_DELAY: string (nullable = true)\n",
      " |-- WEATHER_DELAY: string (nullable = true)\n",
      " |-- NAS_DELAY: string (nullable = true)\n",
      " |-- SECURITY_DELAY: string (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: string (nullable = true)\n",
      " |-- Unnamed: 27: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a93550a-40ba-4db3-860f-e205e00f27fb",
   "metadata": {},
   "source": [
    "Renomeando as colunas para lowercase e retirando a última (não é utilizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdb8ac6-7dcc-424b-827e-fb9be9927669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(\"Unnamed: 27\")\n",
    "\n",
    "for column in df.columns:\n",
    "    df = df.withColumnRenamed(column, column.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd28da8-8d58-45b9-99e0-d26b217583c5",
   "metadata": {},
   "source": [
    "Iremos corrigir os dados no PySpark. Lembrando que a estrutura DataFrame é lazy, então o código abaixo só irá guardar as operações, que serão feitas na hora da escrita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a414ed78-d138-49a6-8a91-265297b96407",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Column, DataFrame\n",
    "\n",
    "def cast_floatstr_to_int(col_str: str) -> Column:\n",
    "    return F.col(col_str).cast(FloatType()).cast(IntegerType())\n",
    "\n",
    "def datediff_min(col1: str, col2: str) -> Column:\n",
    "    return (F.unix_timestamp(F.col(col1)) - F.unix_timestamp(F.col(col2))) / 60\n",
    "\n",
    "def correct_data(df: DataFrame) -> DataFrame:\n",
    "    df_new = df \\\n",
    "        .withColumn(\"crs_dep_time\", F.concat_ws(' ', F.col(\"fl_date\"), F.concat(F.lpad(F.col(\"crs_dep_time\"), 4, '0'), F.lit(\"00\")))) \\\n",
    "        .withColumn(\"dep_time\", F.concat_ws(' ', F.col(\"fl_date\"), F.concat(F.lpad(cast_floatstr_to_int(\"dep_time\"), 4, '0'), F.lit(\"00\")))) \\\n",
    "        .withColumn(\"crs_arr_time\", F.concat_ws(' ', F.col(\"fl_date\"), F.concat(F.lpad(F.col(\"crs_arr_time\"), 4, '0'), F.lit(\"00\")))) \\\n",
    "        .withColumn(\"arr_time\", F.concat_ws(' ', F.col(\"fl_date\"), F.concat(F.lpad(cast_floatstr_to_int(\"arr_time\"), 4, '0'), F.lit(\"00\")))) \\\n",
    "        .withColumn(\"wheels_off\", F.concat_ws(' ', F.col(\"fl_date\"), F.concat(F.lpad(cast_floatstr_to_int(\"wheels_off\"), 4, '0'), F.lit(\"00\")))) \\\n",
    "        .withColumn(\"wheels_on\", F.concat_ws(' ', F.col(\"fl_date\"), F.concat(F.lpad(cast_floatstr_to_int(\"wheels_on\"), 4, '0'), F.lit(\"00\")))) \\\n",
    "        .withColumn(\"fl_date\", F.col(\"fl_date\").cast(\"date\")) \\\n",
    "        .withColumn(\"dep_delay\", cast_floatstr_to_int(\"dep_delay\")) \\\n",
    "        .withColumn(\"arr_delay\", cast_floatstr_to_int(\"arr_delay\")) \\\n",
    "        .withColumn(\"taxi_out\", cast_floatstr_to_int(\"taxi_out\")) \\\n",
    "        .withColumn(\"taxi_in\", cast_floatstr_to_int(\"taxi_in\"))\n",
    "    \n",
    "    timestamp_cols = [\"wheels_on\", \"wheels_off\", \"arr_time\", \"crs_arr_time\", \"dep_time\", \"crs_dep_time\"]\n",
    "    \n",
    "    for col in timestamp_cols:\n",
    "        df_new = df_new.withColumn(col, F.to_timestamp(col, \"yyyy-MM-dd HHmmss\"))\n",
    "    \n",
    "    # check if dates are in the other day\n",
    "    df_new = df_new \\\n",
    "        .withColumn(\"dep_time\", F.when(datediff_min(\"crs_dep_time\", \"dep_time\") > F.abs(F.col(\"dep_delay\")), F.col(\"dep_time\") + F.expr(\"INTERVAL 1 DAY\")).otherwise(F.col(\"dep_time\"))) \\\n",
    "        .withColumn(\"crs_arr_time\", F.when(datediff_min(\"crs_arr_time\", \"crs_dep_time\") < 0, F.col(\"crs_arr_time\") + F.expr(\"INTERVAL 1 DAY\")).otherwise(F.col(\"crs_arr_time\"))) \\\n",
    "        .withColumn(\"wheels_on\", F.when(datediff_min(\"wheels_on\", \"wheels_off\") < 0, F.col(\"wheels_on\") + F.expr(\"INTERVAL 1 DAY\")).otherwise(F.col(\"wheels_on\"))) \\\n",
    "        .withColumn(\"arr_time\", F.when(F.datediff(F.to_date(F.col(\"crs_arr_time\")), F.to_date(F.col(\"crs_dep_time\"))) > 0, F.col(\"arr_time\") + F.expr(\"INTERVAL 1 DAY\")).otherwise(F.col(\"arr_time\")))\n",
    "    \n",
    "    df_new = df_new \\\n",
    "            .withColumn(\"air_time\", datediff_min(\"wheels_on\", \"wheels_off\")) \\\n",
    "            .withColumn(\"crs_elapsed_time\", datediff_min(\"crs_arr_time\", \"crs_dep_time\")) \\\n",
    "            .withColumn(\"actual_elapsed_time\", F.col(\"air_time\") + F.col(\"taxi_out\") + F.col(\"taxi_in\")) \\\n",
    "            .withColumn(\"distance\", F.col(\"distance\").cast(FloatType())) \\\n",
    "            .withColumn(\"carrier_delay\", F.col(\"carrier_delay\").cast(FloatType())) \\\n",
    "            .withColumn(\"weather_delay\", F.col(\"weather_delay\").cast(FloatType())) \\\n",
    "            .withColumn(\"nas_delay\", F.col(\"nas_delay\").cast(FloatType())) \\\n",
    "            .withColumn(\"security_delay\", F.col(\"security_delay\").cast(FloatType())) \\\n",
    "            .withColumn(\"late_aircraft_delay\", F.col(\"late_aircraft_delay\").cast(FloatType())) \\\n",
    "            .withColumn(\"cancelled\", cast_floatstr_to_int(\"cancelled\").cast(BooleanType())) \\\n",
    "            .withColumn(\"diverted\", cast_floatstr_to_int(\"diverted\").cast(BooleanType()))\n",
    "            \n",
    "    return df_new\n",
    "\n",
    "\n",
    "df = correct_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fe1928a-fc30-42bc-b730-a11e982fc989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- fl_date: date (nullable = true)\n",
      " |-- op_carrier: string (nullable = true)\n",
      " |-- op_carrier_fl_num: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- crs_dep_time: timestamp (nullable = true)\n",
      " |-- dep_time: timestamp (nullable = true)\n",
      " |-- dep_delay: integer (nullable = true)\n",
      " |-- taxi_out: integer (nullable = true)\n",
      " |-- wheels_off: timestamp (nullable = true)\n",
      " |-- wheels_on: timestamp (nullable = true)\n",
      " |-- taxi_in: integer (nullable = true)\n",
      " |-- crs_arr_time: timestamp (nullable = true)\n",
      " |-- arr_time: timestamp (nullable = true)\n",
      " |-- arr_delay: integer (nullable = true)\n",
      " |-- cancelled: boolean (nullable = true)\n",
      " |-- cancellation_code: string (nullable = true)\n",
      " |-- diverted: boolean (nullable = true)\n",
      " |-- crs_elapsed_time: double (nullable = true)\n",
      " |-- actual_elapsed_time: double (nullable = true)\n",
      " |-- air_time: double (nullable = true)\n",
      " |-- distance: float (nullable = true)\n",
      " |-- carrier_delay: float (nullable = true)\n",
      " |-- weather_delay: float (nullable = true)\n",
      " |-- nas_delay: float (nullable = true)\n",
      " |-- security_delay: float (nullable = true)\n",
      " |-- late_aircraft_delay: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14848cdc-7555-4037-944e-c59b5b87afec",
   "metadata": {},
   "source": [
    "Escrevendo para um arquivo Parquet para comprimir e utiizar menos espaço no filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36b90d4a-2dae-4adc-8b6b-eb13690f4e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/06 10:02:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:========================================================>(57 + 1) / 58]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:05:51.123187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "df \\\n",
    "    .write \\\n",
    "    .parquet(str(DATA_PATH / \"flight_delays\") , mode=\"overwrite\")\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a97209-24da-4be0-8142-bf0f012a0574",
   "metadata": {},
   "source": [
    "Deletando os csvs e relendo o DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ff40463-b800-4771-bd20-182235ca7121",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "799264d3-1357-456d-882e-beeb9925e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(str(DATA_PATH / \"flight_delays\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dd4929-f7ac-49af-9f21-c5739823f48e",
   "metadata": {},
   "source": [
    "Carregando os dados no Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf4f6358-dd13-4a62-9f9d-e501bd6a9718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:18:52.826389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "properties = {\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"spark\",\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "url = \"jdbc:postgresql://demo-database:5432/postgres\"\n",
    "\n",
    "t0 = datetime.now()\n",
    "df \\\n",
    "    .write \\\n",
    "    .jdbc(url=url, table=\"flight_delays\", mode=\"append\", properties=properties)\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc653d61-9d11-4fba-ab1f-c5c7c3ba2235",
   "metadata": {},
   "source": [
    "Conectando na database Postgres para comparar as operações do Spark com ela:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4650f431-abc6-4827-82d2-f8583f492757",
   "metadata": {},
   "source": [
    "Conectando no Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07cf014e-bcfa-42fd-87d3-7af204cc034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as psql\n",
    "\n",
    "properties_psql = {\n",
    "    \"database\": \"postgres\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"spark\",\n",
    "    \"host\": \"demo-database\",\n",
    "    \"port\": \"5432\"\n",
    "}\n",
    "\n",
    "conn = psql.connect(**properties_psql)\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea13ed4-40a5-438c-b569-6213bb3274ea",
   "metadata": {},
   "source": [
    "Tempo médio de atraso de saída e de taxi, por aeroporto de saída e por companhia aérea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c394ee7-3bac-4cb6-8186-4e35ef83a862",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:00:05.096790\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "result_spark = df \\\n",
    "    .select(F.col(\"op_carrier\"), F.col(\"origin\"), F.col(\"dep_delay\"), F.col(\"taxi_out\")) \\\n",
    "    .groupBy(\"op_carrier\", \"origin\") \\\n",
    "    .agg(\n",
    "        F.avg(\"dep_delay\").alias(\"avg_dep_delay\"),\n",
    "        F.avg(\"taxi_out\").alias(\"avg_taxi_out\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"avg_dep_delay\").desc(), F.col(\"avg_taxi_out\").desc()) \\\n",
    "    .collect()\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22920c83-a97d-4b5e-8c89-47b5e4018e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:00:11.316320\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    select\n",
    "        origin,\n",
    "        op_carrier,\n",
    "        avg(dep_delay) as avg_dep_delay,\n",
    "        avg(taxi_out) as avg_taxi_out\n",
    "    from\n",
    "        flight_delays\n",
    "    group by\n",
    "        origin,\n",
    "        op_carrier\n",
    "    order by\n",
    "        3 desc,\n",
    "        4 desc\n",
    "    \"\"\"\n",
    ")\n",
    "result_postgres = cur.fetchall()\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4424ed8d-fc98-4605-bba6-2688bd5b8530",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=====================================================>  (18 + 1) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+------------------+------------------+\n",
      "|op_carrier|origin|     avg_dep_delay|      avg_taxi_out|\n",
      "+----------+------+------------------+------------------+\n",
      "|        F9|   OAK|             328.0|              13.0|\n",
      "|        OO|   ENV|             157.0|               8.0|\n",
      "|        EV|   BZN|             142.0|              17.0|\n",
      "|        VX|   TUL|             113.0|              11.0|\n",
      "|        EV|   BOI|             113.0|               8.0|\n",
      "|        EV|   RKS|             105.0|               5.0|\n",
      "|        G4|   CPR|             100.0|               7.0|\n",
      "|        B6|   AVP|              91.0|              25.0|\n",
      "|        OO|   GCK|              74.0|              14.0|\n",
      "|        9E|   GTF| 71.73684210526316|20.789473684210527|\n",
      "|        EV|   MCN|              67.0|               1.0|\n",
      "|        G4|   YNG|              63.0|              38.5|\n",
      "|        OO|   SWO|              61.0|               8.0|\n",
      "|        XE|   SPI|              61.0|               5.0|\n",
      "|        G4|   AVP|              60.5|              12.0|\n",
      "|        UA|   AGS|              60.0|              11.2|\n",
      "|        EV|   CYS|              60.0|               7.0|\n",
      "|        EV|   HTS|              58.0|              9.75|\n",
      "|        UA|   AVL|57.808664259927795|16.469314079422382|\n",
      "|        OO|   CLL|        57.7734375|        15.6015625|\n",
      "+----------+------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select(F.col(\"op_carrier\"), F.col(\"origin\"), F.col(\"dep_delay\"), F.col(\"taxi_out\")) \\\n",
    "    .groupBy(\"op_carrier\", \"origin\") \\\n",
    "    .agg(\n",
    "        F.avg(\"dep_delay\").alias(\"avg_dep_delay\"),\n",
    "        F.avg(\"taxi_out\").alias(\"avg_taxi_out\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"avg_dep_delay\").desc(), F.col(\"avg_taxi_out\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d052a25-051b-44ee-b6c5-a09ff7d792d0",
   "metadata": {},
   "source": [
    "Média de tempo de atraso total, atraso de partida e de chegada e de tempos de taxi, por companhia aérea, aeroporto de origem e de destino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b79af2a-6785-42dc-a673-3db7c085264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:00:10.256303\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "result_spark = df \\\n",
    "    .select(\n",
    "        F.col(\"op_carrier\"),\n",
    "        F.col(\"origin\"),\n",
    "        F.col(\"dest\"),\n",
    "        F.col(\"crs_elapsed_time\"),\n",
    "        F.col(\"actual_elapsed_time\"),\n",
    "        F.col(\"dep_delay\"),\n",
    "        F.col(\"taxi_out\"),\n",
    "        F.col(\"taxi_in\"),\n",
    "        F.col(\"arr_delay\")\n",
    "    ) \\\n",
    "    .groupBy(\"op_carrier\", \"origin\", \"dest\") \\\n",
    "    .agg(\n",
    "        F.avg(F.col(\"crs_elapsed_time\") - F.col(\"actual_elapsed_time\")).alias(\"avg_total_delay\"),\n",
    "        F.avg(\"dep_delay\").alias(\"avg_dep_delay\"),\n",
    "        F.avg(\"taxi_out\").alias(\"avg_taxi_out\"),\n",
    "        F.avg(\"taxi_in\").alias(\"avg_taxi_in\"),\n",
    "        F.avg(\"arr_delay\").alias(\"avg_arr_delay\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"avg_total_delay\").desc()) \\\n",
    "    .collect()\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01c6be70-1bf4-476d-97af-4640c7561b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0:00:22.466811\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    with total_delay as (\n",
    "        select\n",
    "            origin,\n",
    "            dest,\n",
    "            op_carrier,\n",
    "            extract(epoch from (crs_elapsed_time - actual_elapsed_time))/60 as total_delay,\n",
    "            dep_delay,\n",
    "            arr_delay,\n",
    "            taxi_out,\n",
    "            taxi_in\n",
    "        from\n",
    "            flight_delays\n",
    "    )\n",
    "    select\n",
    "        origin,\n",
    "        dest,\n",
    "        op_carrier,\n",
    "        avg(total_delay) as avg_total_delay,\n",
    "        avg(dep_delay) as avg_dep_delay,\n",
    "        avg(arr_delay) as avg_arr_delay,\n",
    "        avg(taxi_out) as avg_taxi_out,\n",
    "        avg(taxi_in) as avg_taxi_in\n",
    "    from\n",
    "        total_delay\n",
    "    group by\n",
    "        origin,\n",
    "        dest,\n",
    "        op_carrier\n",
    "    order by\n",
    "        4 desc\n",
    "    \"\"\"\n",
    ")\n",
    "result_postgres = cur.fetchall()\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51635e6c-2f06-4298-8b73-55b3940ce1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+------------------+-------------------+------------------+------------------+------------------+\n",
      "|op_carrier|origin|dest|   avg_total_delay|      avg_dep_delay|      avg_taxi_out|       avg_taxi_in|     avg_arr_delay|\n",
      "+----------+------+----+------------------+-------------------+------------------+------------------+------------------+\n",
      "|        AA|   CLE| ORD|            1482.0|               -5.0|              16.0|              37.0|              -7.0|\n",
      "|        UA|   LAX| MIA|            1480.0|               -3.0|              15.0|              37.0|              -8.0|\n",
      "|        OO|   PDX| OKC|            1477.0|              -32.0|               9.0|               5.0|             -27.0|\n",
      "|        OH|   ATL| DHN|1460.9411764705883| 29.205882352941178|27.529411764705884| 5.117647058823529| 28.41176470588235|\n",
      "|        UA|   LAX| CMH|            1459.0|                0.0|              17.0|              19.0|             -13.0|\n",
      "|        EV|   MEM| JFK|            1457.0|                3.0|              15.0|              20.0|              13.0|\n",
      "|        OO|   LAX| CID|            1456.0|               -3.0|              26.0|              13.0|              -3.0|\n",
      "|        EV|   ISN| BIL|            1454.0|              -10.0|              13.0|               4.0|               4.0|\n",
      "|        AA|   DEN| STL|            1454.0|               -6.0|              23.0|               8.0|               7.0|\n",
      "|        YV|   LAS| DFW|            1454.0|               -5.0|              25.0|               9.0|               5.0|\n",
      "|        NK|   PDX| DTW| 1453.103448275862|-2.8620689655172415|12.793103448275861|10.241379310344827|-7.137931034482759|\n",
      "|        XE|   MBS| ORD|            1453.0|               -8.0|              20.0|               5.0|              -9.0|\n",
      "|        VX|   JFK| SLC|            1453.0|               -2.0|              35.0|              11.0|              null|\n",
      "|        OH|   DTW| MKE|            1453.0|               -5.0|              50.0|               8.0|              23.0|\n",
      "|        UA|   PSP| EWR| 1452.076923076923|-2.5384615384615383|12.538461538461538| 9.538461538461538|-14.76923076923077|\n",
      "|        YV|   AGS| ATL|            1452.0|               -6.0|              14.0|               6.0|             -19.0|\n",
      "|        FL|   LGA| LAS|            1452.0|               36.0|              13.0|              48.0|              66.0|\n",
      "|        9E|   DTW| ORD|1451.5495978552278| 15.182561307901908|22.211796246648795|10.375335120643431|11.344173441734418|\n",
      "|        FL|   IND| MKE|1451.1935483870968| -3.596774193548387|13.806451612903226| 7.596774193548387|-4.080645161290323|\n",
      "|        9E|   ATL| PFN|            1451.0|               -9.0|              20.0|               2.0|             -22.0|\n",
      "+----------+------+----+------------------+-------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select(\n",
    "        F.col(\"op_carrier\"),\n",
    "        F.col(\"origin\"),\n",
    "        F.col(\"dest\"),\n",
    "        F.col(\"crs_elapsed_time\"),\n",
    "        F.col(\"actual_elapsed_time\"),\n",
    "        F.col(\"dep_delay\"),\n",
    "        F.col(\"taxi_out\"),\n",
    "        F.col(\"taxi_in\"),\n",
    "        F.col(\"arr_delay\")\n",
    "    ) \\\n",
    "    .where(F.col(\"cancelled\") == False) \\\n",
    "    .groupBy(\"op_carrier\", \"origin\", \"dest\") \\\n",
    "    .agg(\n",
    "        F.avg(F.col(\"actual_elapsed_time\") - F.col(\"crs_elapsed_time\")).alias(\"avg_total_delay\"),\n",
    "        F.avg(\"dep_delay\").alias(\"avg_dep_delay\"),\n",
    "        F.avg(\"taxi_out\").alias(\"avg_taxi_out\"),\n",
    "        F.avg(\"taxi_in\").alias(\"avg_taxi_in\"),\n",
    "        F.avg(\"arr_delay\").alias(\"avg_arr_delay\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"avg_total_delay\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9850488c-7272-4902-8d8e-bfb1f6666049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:================================================>       (12 + 2) / 14]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+-------------------+-------------------+-------------------+-------------------+--------+----------------+-------------------+---------+--------+-------+---------+---------+-----------+\n",
      "|op_carrier|origin|dest|       crs_dep_time|       crs_arr_time|         wheels_off|          wheels_on|air_time|crs_elapsed_time|actual_elapsed_time|dep_delay|taxi_out|taxi_in|arr_delay|cancelled|total_delay|\n",
      "+----------+------+----+-------------------+-------------------+-------------------+-------------------+--------+----------------+-------------------+---------+--------+-------+---------+---------+-----------+\n",
      "|        AA|   CLE| ORD|2018-11-24 07:48:00|2018-11-24 08:35:00|2018-11-24 07:59:00|2018-11-25 08:35:00|  1476.0|            47.0|             1529.0|       -5|      16|     37|       -7|    false|    -1482.0|\n",
      "+----------+------+----+-------------------+-------------------+-------------------+-------------------+--------+----------------+-------------------+---------+--------+-------+---------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .select(\n",
    "        F.col(\"op_carrier\"),\n",
    "        F.col(\"origin\"),\n",
    "        F.col(\"dest\"),\n",
    "        F.col(\"crs_dep_time\"),\n",
    "        F.col(\"crs_arr_time\"),\n",
    "        F.col(\"wheels_off\"),\n",
    "        F.col(\"wheels_on\"),\n",
    "        F.col(\"air_time\"),\n",
    "        F.col(\"crs_elapsed_time\"),\n",
    "        F.col(\"actual_elapsed_time\"),\n",
    "        F.col(\"dep_delay\"),\n",
    "        F.col(\"taxi_out\"),\n",
    "        F.col(\"taxi_in\"),\n",
    "        F.col(\"arr_delay\"),\n",
    "        F.col(\"cancelled\")\n",
    "    ) \\\n",
    "    .withColumn(\"total_delay\", F.col(\"crs_elapsed_time\") - F.col(\"actual_elapsed_time\")) \\\n",
    "    .where((F.col(\"op_carrier\") == \"AA\") & (F.col(\"origin\") == \"CLE\") & (F.col(\"dest\") == \"ORD\")) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467a2174-35b6-4af1-b6b4-e2c79f5dfad0",
   "metadata": {},
   "source": [
    "Essa daqui dá crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c976378-0394-4fab-aa31-b08d9e80740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"refresh progress\" java.lang.OutOfMemoryError: Java heap space\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.status.AppStatusStore.activeStages(AppStatusStore.scala:114)\n",
      "\tat org.apache.spark.ui.ConsoleProgressBar.org$apache$spark$ui$ConsoleProgressBar$$refresh(ConsoleProgressBar.scala:64)\n",
      "\tat org.apache.spark.ui.ConsoleProgressBar$$anon$1.run(ConsoleProgressBar.scala:52)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Unknown Source)\n",
      "\tat java.base/java.util.TimerThread.run(Unknown Source)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o157.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:374)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:411)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:410)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2992/0x000000084120c840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:410)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:340)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2526/0x0000000841032840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:368)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:340)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\n\tat org.apache.spark.sql.Dataset$$Lambda$2200/0x0000000840eb5040.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.Dataset$$Lambda$2201/0x0000000840eb5840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2206/0x0000000840eb8040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2202/0x0000000840eb5c40.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m t0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m----> 2\u001b[0m result_spark \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfl_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mop_carrier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morigin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrs_elapsed_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactual_elapsed_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdep_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marr_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfl_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mop_carrier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43morigin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcrs_elapsed_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mactual_elapsed_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_total_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdep_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_dep_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_taxi_out\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaxi_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_taxi_in\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marr_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_arr_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morderBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mavg_total_delay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m t1 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m     26\u001b[0m delta \u001b[38;5;241m=\u001b[39m t1 \u001b[38;5;241m-\u001b[39m t0\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/dataframe.py:693\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    684\u001b[0m \n\u001b[1;32m    685\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m[Row(age=2, name='Alice'), Row(age=5, name='Bob')]\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc) \u001b[38;5;28;01mas\u001b[39;00m css:\n\u001b[0;32m--> 693\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(PickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o157.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1._next(SparkPlan.scala:377)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:388)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.getNext(SparkPlan.scala:374)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1(SparkPlan.scala:411)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollect$1$adapted(SparkPlan.scala:410)\n\tat org.apache.spark.sql.execution.SparkPlan$$Lambda$2992/0x000000084120c840.apply(Unknown Source)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:410)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:340)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec$$Lambda$2526/0x0000000841032840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:368)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:340)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:3538)\n\tat org.apache.spark.sql.Dataset$$Lambda$2200/0x0000000840eb5040.apply(Unknown Source)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3706)\n\tat org.apache.spark.sql.Dataset$$Lambda$2201/0x0000000840eb5840.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2206/0x0000000840eb8040.apply(Unknown Source)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.execution.SQLExecution$$$Lambda$2202/0x0000000840eb5c40.apply(Unknown Source)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3535)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n"
     ]
    }
   ],
   "source": [
    "t0 = datetime.now()\n",
    "result_spark = df \\\n",
    "    .select(\n",
    "        F.col(\"fl_date\"),\n",
    "        F.col(\"op_carrier\"),\n",
    "        F.col(\"origin\"),\n",
    "        F.col(\"dest\"),\n",
    "        F.col(\"crs_elapsed_time\"),\n",
    "        F.col(\"actual_elapsed_time\"),\n",
    "        F.col(\"dep_delay\"),\n",
    "        F.col(\"taxi_out\"),\n",
    "        F.col(\"taxi_in\"),\n",
    "        F.col(\"arr_delay\")\n",
    "    ) \\\n",
    "    .groupBy(\"fl_date\", \"op_carrier\", \"origin\", \"dest\") \\\n",
    "    .agg(\n",
    "        F.avg(F.col(\"crs_elapsed_time\") - F.col(\"actual_elapsed_time\")).alias(\"avg_total_delay\"),\n",
    "        F.avg(\"dep_delay\").alias(\"avg_dep_delay\"),\n",
    "        F.avg(\"taxi_out\").alias(\"avg_taxi_out\"),\n",
    "        F.avg(\"taxi_in\").alias(\"avg_taxi_in\"),\n",
    "        F.avg(\"arr_delay\").alias(\"avg_arr_delay\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"avg_total_delay\").desc()) \\\n",
    "    .collect()\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e564c70-2d53-4fd4-beb3-439975686778",
   "metadata": {},
   "source": [
    "Essa query no postgres crasha o computador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8440fd-2496-471e-a079-d5a153a3ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = datetime.now()\n",
    "cur.execute(\n",
    "    \"\"\"\n",
    "    select\n",
    "        fl_date,\n",
    "        origin,\n",
    "        dest,\n",
    "        op_carrier,\n",
    "        avg(crs_elapsed_time - actual_elapsed_time) as avg_total_delay,\n",
    "        avg(dep_delay) as avg_dep_delay,\n",
    "        avg(arr_delay) as avg_arr_delay,\n",
    "        avg(taxi_out) as avg_taxi_out,\n",
    "        avg(taxi_in) as avg_taxi_in\n",
    "    from\n",
    "        flight_delays\n",
    "    group by\n",
    "        fl_date,\n",
    "        origin,\n",
    "        dest,\n",
    "        op_carrier\n",
    "    order by\n",
    "        5 desc\n",
    "    \"\"\"\n",
    ")\n",
    "result_postgres = cur.fetchall()\n",
    "t1 = datetime.now()\n",
    "delta = t1 - t0\n",
    "\n",
    "print(f\"Execution time: {delta}\")\n",
    "conn.commit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
